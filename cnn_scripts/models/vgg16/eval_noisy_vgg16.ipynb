{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned VGG16 on noisy test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to evaluate the robustness of the fine-tuned models to noise. Without this, we only know how these fine-tuned models perform on the existing clean test set, but we don't know how it performs on noisy speech samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the project root without relying on __file__\n",
    "notebook_path = os.getcwd()  # Gets current working directory\n",
    "project_root = os.path.abspath(os.path.join(notebook_path, \"../..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed for reproducability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some constant strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = '../../../data/melspectrogram_test_dataset.csv'\n",
    "root_dir = '../../../data/'\n",
    "class_weights_path = '../../../data/class_weights.pt'\n",
    "\n",
    "fine_tuned_weights = 'best_vgg16.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset class for loading the mel spectrogram images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.MelspectrogramDataset import MelSpectrogramDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the testing/evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, criterion, device):\n",
    "    test_loss = 0.0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(testloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for later use\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    test_accuracy = test_correct / test_total * 100\n",
    "\n",
    "    return test_loss, test_accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class to add Gaussian noise to the test mel spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentation.AddGaussianNoise import AddGaussianNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't load ImageNet weights here since we'll load our fine-tuned weights\n",
    "model = models.vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Gaussian noise to the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize to model input size\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(noise_factor_range=(0.2, 0.2)  # Fixed noise factor\n",
    "                     ),  # Use a fixed noise level\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test set, test loader, and device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = MelSpectrogramDataset(\n",
    "    csv_file=test_csv, root_dir=root_dir, transform=test_transform)\n",
    "testloader = DataLoader(testset, batch_size=128,\n",
    "                        shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create figure and axes\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,  # Show numbers in cells\n",
    "                fmt='d',     # Use integer formatting\n",
    "                cmap='Blues',  # Color scheme\n",
    "                xticklabels=testset.label_map.keys(),\n",
    "                yticklabels=testset.label_map.keys())\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Rotate axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to print out the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(true_labels, pred_labels, label_map):\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average='weighted')\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1-score: {f1:.4f}\\n\")\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(classification_report(true_labels, pred_labels,\n",
    "          target_names=list(label_map.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model on the noisy test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.load(class_weights_path).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "num_classes = 9  # Adjust according to your dataset\n",
    "\n",
    "num_features = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Load the fine-tuned weights (change the filename as needed)\n",
    "model.load_state_dict(torch.load(fine_tuned_weights, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model on the noisy test set\n",
    "test_loss, test_accuracy, all_preds, all_labels = test(\n",
    "    model, testloader, criterion, device)\n",
    "\n",
    "# Print detailed evaluation metrics\n",
    "get_evaluation_metrics(all_labels, all_preds, testset.label_map)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
