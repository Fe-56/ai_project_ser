{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec2 transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Wav2Vec2Processor, Trainer, TrainingArguments, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/melspectrogram_train_dataset.csv')\n",
    "df_train = df_train[['Filepath', 'Emotion']]\n",
    "df_test = pd.read_csv('../data/melspectrogram_test_dataset.csv')\n",
    "df_test = df_test[['Filepath', 'Emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Filepath   Emotion\n",
      "0              ./dataset/mlend\\MLEndSND_Public\\12243.wav   Neutral\n",
      "1         ./dataset/crema-d\\AudioWAV\\1075_ITH_NEU_XX.wav   Neutral\n",
      "2         ./dataset/crema-d\\AudioWAV\\1050_TIE_NEU_XX.wav   Neutral\n",
      "3              ./dataset/mlend\\MLEndSND_Public\\10530.wav     Happy\n",
      "4         ./dataset/crema-d\\AudioWAV\\1001_IEO_FEA_LO.wav      Fear\n",
      "...                                                  ...       ...\n",
      "50534                ./dataset/meld\\train\\dia23_utt7.mp4   Neutral\n",
      "50535          ./dataset/tess\\YAF_fear\\YAF_read_fear.wav      Fear\n",
      "50536          ./dataset/mlend\\MLEndSND_Public\\22283.wav     Happy\n",
      "50537          ./dataset/mlend\\MLEndSND_Public\\02938.wav  Question\n",
      "50538  ./dataset/ravdess\\Actor_11\\03-01-06-01-01-01-1...      Fear\n",
      "\n",
      "[50539 rows x 2 columns]\n",
      "                                                Filepath   Emotion\n",
      "0            ./dataset/esd\\0017\\Surprise\\0017_001602.wav  Surprise\n",
      "1              ./dataset/mlend\\MLEndSND_Public\\01848.wav   Neutral\n",
      "2              ./dataset/mlend\\MLEndSND_Public\\34452.wav   Neutral\n",
      "3              ./dataset/mlend\\MLEndSND_Public\\22761.wav     Bored\n",
      "4      ./dataset/ravdess\\Actor_09\\03-01-03-01-02-01-0...     Happy\n",
      "...                                                  ...       ...\n",
      "12630          ./dataset/mlend\\MLEndSND_Public\\12770.wav  Question\n",
      "12631         ./dataset/esd\\0015\\Neutral\\0015_000089.wav   Neutral\n",
      "12632     ./dataset/crema-d\\AudioWAV\\1025_IWL_HAP_XX.wav     Happy\n",
      "12633          ./dataset/mlend\\MLEndSND_Public\\39566.wav     Happy\n",
      "12634  ./dataset/jl-corpus/Raw JL corpus (unchecked a...     Happy\n",
      "\n",
      "[12635 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anger': 0, 'Bored': 1, 'Disgust': 2, 'Fear': 3, 'Happy': 4, 'Neutral': 5, 'Question': 6, 'Sad': 7, 'Surprise': 8}\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integers\n",
    "unique_labels = sorted(df_train['Emotion'].unique())  \n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(label_map)\n",
    "\n",
    "df_train['Emotion'] = df_train['Emotion'].map(label_map)\n",
    "df_test['Emotion'] = df_test['Emotion'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Filepath  Emotion\n",
      "0              ./dataset/mlend\\MLEndSND_Public\\12243.wav        5\n",
      "1         ./dataset/crema-d\\AudioWAV\\1075_ITH_NEU_XX.wav        5\n",
      "2         ./dataset/crema-d\\AudioWAV\\1050_TIE_NEU_XX.wav        5\n",
      "3              ./dataset/mlend\\MLEndSND_Public\\10530.wav        4\n",
      "4         ./dataset/crema-d\\AudioWAV\\1001_IEO_FEA_LO.wav        3\n",
      "...                                                  ...      ...\n",
      "50534                ./dataset/meld\\train\\dia23_utt7.mp4        5\n",
      "50535          ./dataset/tess\\YAF_fear\\YAF_read_fear.wav        3\n",
      "50536          ./dataset/mlend\\MLEndSND_Public\\22283.wav        4\n",
      "50537          ./dataset/mlend\\MLEndSND_Public\\02938.wav        6\n",
      "50538  ./dataset/ravdess\\Actor_11\\03-01-06-01-01-01-1...        3\n",
      "\n",
      "[50539 rows x 2 columns]\n",
      "                                                Filepath  Emotion\n",
      "0            ./dataset/esd\\0017\\Surprise\\0017_001602.wav        8\n",
      "1              ./dataset/mlend\\MLEndSND_Public\\01848.wav        5\n",
      "2              ./dataset/mlend\\MLEndSND_Public\\34452.wav        5\n",
      "3              ./dataset/mlend\\MLEndSND_Public\\22761.wav        1\n",
      "4      ./dataset/ravdess\\Actor_09\\03-01-03-01-02-01-0...        4\n",
      "...                                                  ...      ...\n",
      "12630          ./dataset/mlend\\MLEndSND_Public\\12770.wav        6\n",
      "12631         ./dataset/esd\\0015\\Neutral\\0015_000089.wav        5\n",
      "12632     ./dataset/crema-d\\AudioWAV\\1025_IWL_HAP_XX.wav        4\n",
      "12633          ./dataset/mlend\\MLEndSND_Public\\39566.wav        4\n",
      "12634  ./dataset/jl-corpus/Raw JL corpus (unchecked a...        4\n",
      "\n",
      "[12635 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class SpeechEmotionDataset(Dataset):\n",
    "    # Max_length = 4s, 64000 because sampling rate is 16000\n",
    "    def __init__(self, df, processor, max_length=64000):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = '../data/' + self.df.iloc[idx]['Filepath']\n",
    "        label = self.df.iloc[idx]['Emotion']\n",
    "        \n",
    "        # Load audio file\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Pad speech to required length\n",
    "        speech = np.pad(speech, (0, self.max_length - len(speech)), mode='constant')\n",
    "        \n",
    "        # Preprocess audio\n",
    "        inputs = self.processor(speech, sampling_rate=16000, return_tensors='pt', padding=True, truncate=True, max_length=self.max_length)\n",
    "        \n",
    "        input_values = inputs.input_values.squeeze()\n",
    "        return {'input_values': input_values, 'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base')\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base', num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = SpeechEmotionDataset(df_train, processor)\n",
    "test_dataset = SpeechEmotionDataset(df_test, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `truncate` is not a valid argument for this processor and will be ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([-0.0078,  0.0006,  0.0134,  ...,  0.0013,  0.0013,  0.0013]),\n",
       " 'labels': tensor(5)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir='./models/wav2vec2_base',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for computing metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids # original labels\n",
    "    preds = np.argmax(pred.predictions, axis=1) # model predicted labels\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\profi\\AppData\\Local\\Temp\\ipykernel_11988\\1804329444.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  speech, sr = librosa.load(audio_path, sr=16000)\n",
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9477' max='9477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9477/9477 47:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.550396</td>\n",
       "      <td>0.816146</td>\n",
       "      <td>0.827688</td>\n",
       "      <td>0.816146</td>\n",
       "      <td>0.816181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.451933</td>\n",
       "      <td>0.851682</td>\n",
       "      <td>0.854715</td>\n",
       "      <td>0.851682</td>\n",
       "      <td>0.850907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.423628</td>\n",
       "      <td>0.868302</td>\n",
       "      <td>0.872039</td>\n",
       "      <td>0.868302</td>\n",
       "      <td>0.868140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\profi\\AppData\\Local\\Temp\\ipykernel_11988\\1804329444.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  speech, sr = librosa.load(audio_path, sr=16000)\n",
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "C:\\Users\\profi\\AppData\\Local\\Temp\\ipykernel_11988\\1804329444.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  speech, sr = librosa.load(audio_path, sr=16000)\n",
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9477, training_loss=0.5742961291435639, metrics={'train_runtime': 2871.0612, 'train_samples_per_second': 52.809, 'train_steps_per_second': 3.301, 'total_flos': 5.506004919364992e+18, 'train_loss': 0.5742961291435639, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\profi\\AppData\\Local\\Temp\\ipykernel_11988\\1804329444.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  speech, sr = librosa.load(audio_path, sr=16000)\n",
      "c:\\Users\\profi\\OneDrive\\Desktop\\AI-Project--Speech-Emotion-Recognition\\myenv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [790/790 02:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4236275851726532, 'eval_accuracy': 0.8683023347843293, 'eval_precision': 0.8720393979981969, 'eval_recall': 0.8683023347843293, 'eval_f1': 0.8681400892474087, 'eval_runtime': 123.1622, 'eval_samples_per_second': 102.588, 'eval_steps_per_second': 6.414, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract logged metrics\n",
    "def extract_metrics(log_history):\n",
    "    metrics = {\n",
    "        'epochs': [],\n",
    "        'train_loss': [],\n",
    "        'eval_loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    for log in log_history:\n",
    "        # Training loss\n",
    "        if 'loss' in log and 'epoch' in log:\n",
    "            metrics['epochs'].append(log['epoch'])\n",
    "            metrics['train_loss'].append(log.get('loss', None))\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        if 'eval_loss' in log:\n",
    "            metrics['eval_loss'].append(log.get('eval_loss', None))\n",
    "            metrics['accuracy'].append(log.get('eval_accuracy', None))\n",
    "            metrics['precision'].append(log.get('eval_precision', None))\n",
    "            metrics['recall'].append(log.get('eval_recall', None))\n",
    "            metrics['f1'].append(log.get('eval_f1', None))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Extract metrics\n",
    "extracted_metrics = extract_metrics(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss Plot\n",
    "    ax1.plot(metrics['epochs'], metrics['train_loss'], label='Training Loss', color='blue')\n",
    "    ax1.plot(metrics['epochs'], metrics['eval_loss'], label='Validation Loss', color='red')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Metrics Plot\n",
    "    ax2.plot(metrics['epochs'], metrics['accuracy'], label='Accuracy', color='green')\n",
    "    ax2.plot(metrics['epochs'], metrics['precision'], label='Precision', color='blue')\n",
    "    ax2.plot(metrics['epochs'], metrics['recall'], label='Recall', color='red')\n",
    "    ax2.plot(metrics['epochs'], metrics['f1'], label='F1-score', color='purple')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Evaluation Metrics')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(extracted_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)  # Convert logits to predicted class labels\n",
    "true_labels = predictions.label_ids  # Ground truth labels\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Anger', 1: 'Bored', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Neutral', 6: 'Question', 7: 'Sad', 8: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "inv_label_map = {idx: label for label, idx in label_map.items()}\n",
    "print(inv_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label: Happy\n",
      "Predicted Label: Happy\n"
     ]
    }
   ],
   "source": [
    "idx = random.randrange(0, len(test_dataset))\n",
    "print(\"Original Label:\", inv_label_map[int(test_dataset[idx]['labels'])])\n",
    "input_values = test_dataset[idx]['input_values'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax(dim=-1).item()\n",
    "print('Predicted Label:', inv_label_map[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label: Neutral\n",
      "Predicted Label: Neutral\n"
     ]
    }
   ],
   "source": [
    "idx = random.randrange(0, len(test_dataset))\n",
    "print(\"Original Label:\", inv_label_map[int(test_dataset[idx]['labels'])])\n",
    "input_values = test_dataset[idx]['input_values'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax(dim=-1).item()\n",
    "print('Predicted Label:', inv_label_map[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label: Anger\n",
      "Predicted Label: Anger\n"
     ]
    }
   ],
   "source": [
    "idx = random.randrange(0, len(test_dataset))\n",
    "print(\"Original Label:\", inv_label_map[int(test_dataset[idx]['labels'])])\n",
    "input_values = test_dataset[idx]['input_values'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax(dim=-1).item()\n",
    "print('Predicted Label:', inv_label_map[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
