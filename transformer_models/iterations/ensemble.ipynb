{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import set_seed, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, WavLMForSequenceClassification, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_path, '../..'))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('../../data/val_dataset.csv')\n",
    "df_val = df_val[['Filepath', 'Emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers\n",
    "unique_labels = sorted(df_val['Emotion'].unique())\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(label_map)\n",
    "\n",
    "df_val['Emotion'] = df_val['Emotion'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model 1: facebook/wav2vec2-base\n",
    "model1_checkpoint_path = '../models/wav2vec2-base_standardpad/checkpoint-XXXX'\n",
    "processor1 = Wav2Vec2Processor.from_pretrained(model1_checkpoint_path)\n",
    "model1 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    model1_checkpoint_path, num_labels=len(label_map))\n",
    "\n",
    "# Load Model 2: microsoft/wavlm-base\n",
    "model2_checkpoint_path = '../models/wavlm-base_standardpad/checkpoint-XXXX'\n",
    "processor2 = Wav2Vec2FeatureExtractor.from_pretrained(model2_checkpoint_path)\n",
    "model2 = WavLMForSequenceClassification.from_pretrained(\n",
    "    model2_checkpoint_path, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_models.emotion_datasets.SpeechEmotionDatasetStandardPad import SpeechEmotionDatasetStandardPad\n",
    "\n",
    "# Create two validation datasets, one for each model\n",
    "val_dataset1 = SpeechEmotionDatasetStandardPad(df_val, processor1)\n",
    "val_dataset2 = SpeechEmotionDatasetStandardPad(df_val, processor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to obtain model probabilities\n",
    "def get_model_probs(model, dataset, batch_size=128):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # The dataset returns a dict with 'input_values'\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            outputs = model(input_values).logits  # shape: (B, num_labels)\n",
    "            probs = softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    return np.vstack(all_probs)  # shape: (N, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for each model from their respective datasets\n",
    "probs1 = get_model_probs(model1, val_dataset1, batch_size=128)\n",
    "probs2 = get_model_probs(model2, val_dataset2, batch_size=128)\n",
    "\n",
    "# Stack predictions horizontally\n",
    "X_meta = np.hstack([probs1, probs2])  # shape: (N, num_labels * 2)\n",
    "\n",
    "# Extract ground truth labels from one of the datasets (they should be the same)\n",
    "y_meta = np.array([sample['labels'].item() for sample in val_dataset1])\n",
    "\n",
    "print(\"Shape of X_meta (stacked predictions):\", X_meta.shape)\n",
    "print(\"Shape of y_meta (labels):\", y_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
